# Data Pipeline Documentation

## Overview

The data pipeline transforms raw video frames into model-ready inputs with both spatial and temporal features.

```
Raw Images â†’ Motion Features â†’ Preprocessing â†’ Augmentation â†’ Model Input
   (JPG)         (Temporal)      (Spatial)      (Training)     (Tensors)
```

---

## Phase 1: Dataset Creation

### Directory Structure

```
images/
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ flip/              â† Frames containing page flips
â”‚   â”‚   â”œâ”€â”€ video1_05.jpg
â”‚   â”‚   â”œâ”€â”€ video1_15.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ notflip/           â† Frames without flips
â”‚       â”œâ”€â”€ video1_01.jpg
â”‚       â”œâ”€â”€ video1_02.jpg
â”‚       â””â”€â”€ ...
â””â”€â”€ testing/
    â”œâ”€â”€ flip/
    â””â”€â”€ notflip/
```

### Filename Convention

```
video1_05.jpg
  â”‚    â”‚   â”‚
  â”‚    â”‚   â””â”€ Extension (.jpg)
  â”‚    â””â”€â”€â”€â”€â”€ Frame number (05)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Video ID (video1)
```

**Why This Matters**:
- Video ID groups frames from same sequence
- Frame number enables temporal ordering
- Essential for calculating motion between consecutive frames

### Dataset DataFrame Structure

```python
create_dataset_df(base_path) â†’ DataFrame
```

**Output**:
| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `image_path` | str | Full path to image | `/path/video1_05.jpg` |
| `label` | int | 0=notflip, 1=flip | 1 |
| `video_id` | str | Video identifier | `video1` |
| `frame_number` | int | Frame position | 5 |
| `split` | str | training/testing | `training` |
| `sequence_position` | str | beginning/middle/end | `beginning` |

**Sequence Position Logic**:
```
Frame 0-9:   "beginning"  â† Video start, less likely to have flips
Frame 10-19: "middle"     â† Most action happens here
Frame 20+:   "end"        â† Video conclusion
```

### Dataset Statistics (analyze_dataset)

```python
analyze_dataset(df)
```

**What It Checks**:

1. **Class Balance**
   ```
   Total images: 5,240
   Training: 3,928 (75%)
   Testing:  1,312 (25%)

   Class distribution:
   NotFlip: 3,500 (67%)
   Flip:    1,740 (33%)
   ```

   **Why This Matters**:
   - Imbalanced classes can bias the model
   - If 90% are "notflip", model might just predict "notflip" always
   - We need to monitor this and potentially use class weights

2. **Video-Level Distribution**
   ```
   Unique videos in training: 45
   Unique videos in testing:  15
   ```

   **Why This Matters**:
   - Train and test videos MUST be different (no leakage!)
   - If same video appears in train and test â†’ inflated accuracy

3. **Per-Video Class Balance**
   ```
   video1: NotFlip=25, Flip=5 (5:1 ratio)
   video2: NotFlip=28, Flip=2 (14:1 ratio) â† Highly imbalanced!
   ```

   **Why This Matters**:
   - Some videos may have very few flip frames
   - Model might struggle to learn from imbalanced videos

### Success Criteria for Dataset
- âœ“ No missing files or corrupted images
- âœ“ Reasonable class balance (ideally 30-70% flip frames)
- âœ“ Train/test video separation (no overlap)
- âœ“ Sufficient samples per class (>500 each)

---

## Phase 2: Motion Feature Extraction

### The Core Problem

**Question**: How do we capture the MOTION of a page flip?

**Answer**: Compare consecutive frames to detect changes.

### Motion Extraction Algorithm

```python
extract_optimized_motion_features(current_frame, previous_frame) â†’ [3 features]
```

#### Step-by-Step Process

```
Input: Two consecutive frames
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frame t-1   â”‚      â”‚ Frame t     â”‚
â”‚             â”‚      â”‚             â”‚
â”‚    ðŸ“„       â”‚      â”‚   ðŸ“„ â†’      â”‚  (Page moving)
â”‚             â”‚      â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: Convert to Grayscale
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 480Ã—640Ã—3   â”‚  â†’   â”‚ 480Ã—640Ã—1   â”‚
â”‚ (RGB)       â”‚      â”‚ (Grayscale) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Why grayscale?
â€¢ Motion patterns visible without color
â€¢ 3Ã— faster processing (1 channel vs 3)
â€¢ Reduces memory usage

Step 2: Resize to 64Ã—64
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 480Ã—640     â”‚  â†’   â”‚ 64Ã—64   â”‚
â”‚             â”‚      â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Why resize?
â€¢ Motion patterns visible at lower resolution
â€¢ ~56Ã— fewer pixels (409,600 â†’ 4,096)
â€¢ MUCH faster computation
â€¢ Trade-off: Lose fine details, but motion is still clear

Step 3: Calculate Frame Difference
difference = abs(current - previous)

Example:
Current:    Previous:   Difference:
â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”
â”‚ 200 â”‚  -  â”‚ 180 â”‚  =  â”‚ 20  â”‚
â”‚ 150 â”‚     â”‚ 150 â”‚     â”‚  0  â”‚
â”‚ 100 â”‚     â”‚ 120 â”‚     â”‚ 20  â”‚
â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜

Step 4: Extract Statistics

difference = [
  [20, 0, 15, 5, ...],   â† Row 1
  [0, 0, 25, 30, ...],   â† Row 2
  [10, 5, 0, 0, ...],    â† Row 3
  ...
]

Feature 1: mean_motion = mean(difference)
  = average pixel change across entire frame

  High mean â†’ Lots of movement (flip likely)
  Low mean  â†’ Little movement (no flip)

  Example: mean_motion = 12.5

Feature 2: std_motion = std(difference)
  = variability of motion across frame

  High std â†’ Non-uniform motion (page edge moving)
  Low std  â†’ Uniform motion (camera shake)

  Example: std_motion = 24.3

Feature 3: max_motion = max(difference)
  = maximum pixel change anywhere

  High max â†’ Sharp, localized motion (flip edge)
  Low max  â†’ Gentle, distributed motion

  Example: max_motion = 87.0
```

### Why These 3 Features?

#### 1. Mean Motion (Overall Activity)
```
No Flip:                Page Flip:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           â”‚          â”‚â–ˆâ–“â–’â–‘       â”‚
â”‚           â”‚          â”‚â–ˆâ–“â–’â–‘       â”‚  â† Significant
â”‚           â”‚          â”‚â–ˆâ–“â–’â–‘       â”‚     change
â”‚           â”‚          â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
mean â‰ˆ 2.5             mean â‰ˆ 25.3
```

**Interview Question**: "Why use mean?"
**Answer**: "Mean motion quantifies overall activity. Page flips involve significant pixel changes, resulting in higher mean values compared to static frames."

#### 2. Standard Deviation (Motion Uniformity)
```
Camera Shake:           Page Flip:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â”‚          â”‚â–ˆâ–ˆâ–ˆ        â”‚  â† Non-uniform
â”‚â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â”‚          â”‚â–ˆâ–ˆâ–’        â”‚     (edge moves,
â”‚â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â”‚          â”‚â–ˆâ–‘â–‘        â”‚      center static)
â”‚â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â”‚          â”‚â–‘          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
std â‰ˆ 3.2              std â‰ˆ 28.7
(uniform)              (non-uniform)
```

**Interview Question**: "Why standard deviation?"
**Answer**: "Std captures motion distribution. Page flips have non-uniform motion (edges move more than center), while camera shake or global motion is more uniform. High std indicates localized motion characteristic of flips."

#### 3. Maximum Motion (Peak Intensity)
```
Slow Movement:          Page Flip:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚â–‘â–‘â–‘â–‘       â”‚          â”‚â–ˆâ–ˆâ–ˆ        â”‚  â† Sharp edge
â”‚â–‘â–‘â–‘â–‘       â”‚          â”‚â–ˆâ–ˆâ–ˆ        â”‚     = high max
â”‚           â”‚          â”‚           â”‚
â”‚           â”‚          â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
max â‰ˆ 15               max â‰ˆ 187
```

**Interview Question**: "Why max motion?"
**Answer**: "Max captures peak intensity. Page flips create sharp edges between the flipping page and background, resulting in high local contrast changes. This spike in maximum difference is a strong flip indicator."

### Parallel Processing with Caching

```python
calculate_optimized_motion_features(df, use_cache=True)
```

**Optimization Strategy**:

```
Without Optimization:           With Optimization:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Process videos sequentially    Process videos in parallel
Frame 1 â†’ Frame 2 â†’ Frame 3   Frame 1 â”
  â†“         â†“         â†“                â”œâ†’ Parallel
Video 1   Video 2   Video 3   Frame 2 â”¤
                                       â”‚
Time: ~30 minutes                Frame 3â”˜

                               Time: ~5 minutes

                               Cache to disk:
                               motion_features_cache.npz

                               Next run: Load from cache
                               Time: ~10 seconds âœ“
```

**Why Caching Matters**:
- Motion calculation is expensive (read images, compute diffs)
- Features don't change between runs
- Cache enables fast experimentation with model architectures

---

## Phase 3: Image Preprocessing

### Three Preprocessing Levels

```python
preprocess_image(image, preprocessing_level='basic')
```

#### Level 1: None (Baseline)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Original Image â”‚
â”‚      480Ã—640    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
    Resize only
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     96Ã—96       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
**Use Case**: Baseline comparison, fastest processing

#### Level 2: Basic (Default)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Original Image     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              â”‚   â”‚  â† Extra background
â”‚  â”‚   Content    â”‚   â”‚
â”‚  â”‚              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
    Crop background
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Content    â”‚  â† Focused on relevant area
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
    Resize to 96Ã—96
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    96Ã—96     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
**Use Case**: Default for training, good balance

#### Level 3: Full (Maximum Enhancement)
```
Original â†’ Crop â†’ Enhance â†’ Sharpen â†’ Resize
                    â†“         â†“
                 Contrast   Edges
                   Ã—1.2     Ã—1.1
```
**Use Case**: When image quality is poor, experimental

### Normalization (Critical Step!)

```python
# Apply to ALL images
transform = transforms.Normalize(
    mean=[0.485, 0.456, 0.406],  # RGB channels
    std=[0.229, 0.224, 0.225]
)
```

**What This Does**:
```
Before Normalization:
pixel_value = 150 (range: 0-255)

After Normalization:
pixel_value = (150/255 - 0.485) / 0.229
            = (0.588 - 0.485) / 0.229
            = 0.450

Result: Values roughly in range [-2, +2]
```

**Why These Specific Values?**
- ImageNet dataset statistics (standard in computer vision)
- Neural networks train better with normalized inputs
- Prevents certain channels from dominating

**Interview Question**: "Why normalize?"
**Answer**:
1. **Gradient stability**: Large pixel values (0-255) â†’ large gradients â†’ unstable training
2. **Zero-centered**: Helps with weight initialization and convergence
3. **Standard practice**: Using ImageNet stats enables transfer learning later

---

## Phase 4: Data Augmentation (Training Only)

### Why Augment?

**Problem**: Limited training data â†’ Model memorizes training set
**Solution**: Create variations to improve generalization

```python
class PageFlipDataset(Dataset):
    def __init__(self, ..., augment=True):
        self.augment = augment
```

### Augmentation Techniques

#### 1. Random Rotation (Â±5 degrees)
```
Original:              Augmented:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ðŸ“„     â”‚    â†’     â”‚  ðŸ“„      â”‚  (Rotated 3Â°)
â”‚          â”‚          â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Why small angles?
â€¢ Page flip videos naturally have slight camera angles
â€¢ Too much rotation (>10Â°) would be unrealistic
â€¢ Helps model generalize to different camera positions
```

#### 2. Random Brightness (0.95Ã— to 1.05Ã—)
```
Original:              Augmented:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â– â– â– â–     â”‚    â†’     â”‚  â–“â–“â–“â–“    â”‚  (Slightly darker)
â”‚  â– â– â– â–     â”‚          â”‚  â–“â–“â–“â–“    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Why subtle changes?
â€¢ Different lighting conditions in videos
â€¢ Flash, shadows, ambient light variations
â€¢ Too much change would distort features
```

#### 3. Color Jitter
```python
transforms.ColorJitter(brightness=0.05, contrast=0.05)
```

**Why ONLY During Training?**
```
Training Set:                 Validation/Test Set:
Apply augmentation            NO augmentation
â†“                            â†“
Model sees variations        Evaluate on clean data
â†“                            â†“
Learns robust features       Measure true performance
```

**Interview Question**: "Why not augment validation/test data?"
**Answer**: "Augmentation is for improving generalization during training. For evaluation, we need consistent, unmodified data to measure true model performance. Augmenting test data would give artificially inflated metrics."

---

## Phase 5: DataLoader Configuration

```python
train_loader = DataLoader(
    train_dataset,
    batch_size=128,        # Large batch for stability
    shuffle=True,          # Randomize order
    num_workers=4,         # Parallel data loading
    pin_memory=True,       # Faster GPU transfer
    persistent_workers=True # Keep workers alive
)
```

### Batch Size: Why 128?

```
Small Batch (32):           Large Batch (128):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Noisy gradients            Smoother gradients
Longer training            Faster training
Less memory               More memory
More updates per epoch     Fewer updates per epoch

Trade-off:
32  â†’ Better generalization, slower
128 â†’ Faster training, stable gradients âœ“
256 â†’ Too large for our dataset, might underfit
```

**Our Choice**: 128 is a sweet spot
- Fast training (fewer iterations)
- Stable gradient estimates
- Fits in memory (96Ã—96 images are small)

### Shuffle: Why True for Training?

```
Without Shuffle:             With Shuffle:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Batch 1: video1_01-08      Batch 1: video3_15, video1_02, ...
Batch 2: video1_09-16      Batch 2: video2_04, video5_20, ...
Batch 3: video1_17-24      Batch 3: video1_08, video4_11, ...

Problem: Sequential frames   Solution: Random mix
are very similar            â†“
â†“                          Model sees diverse examples
Model might overfit to      each batch
specific video sequences    â†“
                           Better generalization âœ“
```

### Num Workers: Why 4?

```
num_workers=0:               num_workers=4:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Main Process                Main Process + 4 Workers
   â†“                           â†“
Load Batch 1               Worker 1: Load Batch 1
   â†“                       Worker 2: Load Batch 2
Train on Batch 1           Worker 3: Load Batch 3
   â†“                       Worker 4: Load Batch 4
Load Batch 2                   â†“
   â†“                       Main: Train on Batch 1
Train on Batch 2                â†“
   â†“                       Main: Train on Batch 2 (already loaded!)
...
                           Result: GPU never waits for data
Time: ~20 min              Time: ~8 min âœ“
```

### Pin Memory: Why True?

```
Without pin_memory:          With pin_memory:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CPU Memory (Pageable)       CPU Memory (Pinned)
      â†“                            â†“
Copy to Staging Area        Direct Transfer
      â†“                            â†“
Transfer to GPU             Transfer to GPU

Time: ~15ms per batch       Time: ~5ms per batch âœ“
```

**Trade-off**: Uses slightly more RAM, but much faster GPU transfer

---

## Data Pipeline Success Criteria

### 1. Data Quality Checks
```python
# Check for issues
assert len(df) > 1000, "Dataset too small"
assert df['label'].value_counts()[0] / len(df) < 0.9, "Too imbalanced"
assert df.isna().sum().sum() == 0, "Missing values detected"
```

### 2. Motion Feature Validation
```python
# Visualize motion features by class
plt.hist(df[df['label']==0]['mean_motion'], alpha=0.5, label='Not Flip')
plt.hist(df[df['label']==1]['mean_motion'], alpha=0.5, label='Flip')
```

**Expected Result**: Flip frames should have higher motion values
```
     Not Flip â–‘â–‘â–‘â–‘â–‘â–‘
     Flip     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘

     0     10    20    30    40
          mean_motion â†’
```

### 3. Preprocessing Validation
```python
visualize_preprocessing(df, num_samples=3)
```

**Check**:
- âœ“ Images are properly cropped (no excessive background)
- âœ“ Normalized values in reasonable range
- âœ“ No artifacts or distortions

### 4. DataLoader Performance
```python
# Measure loading speed
start = time.time()
for batch in train_loader:
    pass  # Just loading
print(f"Time: {time.time() - start:.2f}s")
```

**Target**: < 5 seconds to iterate through entire dataset

---

## Common Issues and Solutions

### Issue 1: Motion features all zeros
```python
# Symptom
df['mean_motion'].describe()
# mean: 0.0, std: 0.0

# Cause: Missing previous frames
# Solution: Check video_id grouping
```

### Issue 2: Out of memory during training
```python
# Symptom
RuntimeError: CUDA out of memory

# Solutions:
1. Reduce batch_size: 128 â†’ 64
2. Reduce image_size: 96 â†’ 64
3. Reduce num_workers: 4 â†’ 2
```

### Issue 3: Slow data loading
```python
# Symptom
GPU utilization < 50%

# Causes & Solutions:
1. num_workers=0 â†’ Increase to 4
2. No pin_memory â†’ Enable pin_memory=True
3. No persistent_workers â†’ Enable for faster restarts
```

---

## Next Steps

- Read [Training Strategy Documentation](04_training_strategy.md) for optimization techniques
- Read [Evaluation Metrics Documentation](05_evaluation_metrics.md) for performance analysis
- See [Architecture Documentation](02_architecture.md) for how data flows through the model
